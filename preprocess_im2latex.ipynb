{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1e8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bcb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_formula(image, padding = 30):\n",
    "    # Image: 4 channel image with alpha.\n",
    "    # Convert black pixels to white pixels.\n",
    "    data = np.array(image)\n",
    "    red, green, blue, alpha = data.T\n",
    "    black_areas = (red < 10) & (blue < 10) & (green < 10)\n",
    "    # Convert alpha to white.\n",
    "    data[..., -1] = 255\n",
    "    # Crop a box around the area that contains black pixels.\n",
    "    coords = np.argwhere(black_areas)\n",
    "    x0, y0 = coords.min(axis=0)\n",
    "    x1, y1 = coords.max(axis=0) + 1\n",
    "    # Add padding.\n",
    "    x0 = max(0, x0 - padding)\n",
    "    y0 = max(0, y0 - padding)\n",
    "    x1 = min(image.width, x1 + padding)\n",
    "    y1 = min(image.height, y1 + padding)\n",
    "    image = Image.fromarray(data[y0:y1, x0:x1])\n",
    "    return image.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61e19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderedLaTeXLabelstr2Formula(label: str):\n",
    "    # We're matching \\\\label{...whatever} and removing it\n",
    "    label = re.sub(r\"\\\\label\\{[^\\}]*\\}\", \"\", label)\n",
    "    # We match \\, and remove it.\n",
    "    label = re.sub(r\"\\\\,\", \"\", label)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03bfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_formula(latex: str):\n",
    "    # Remove \\mbox{...} - not supported by the inline MathJax renderer\n",
    "    parsed_latex = re.sub(r\"\\\\mbox\\{[^\\}]*\\}\", \"\", latex)\n",
    "    display(Math(parsed_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2fc401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--gpu GPU]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\prana\\AppData\\Roaming\\jupyter\\runtime\\kernel-021287d3-cbdb-445a-ba44-1ffaace27713.json\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torchvision.transforms.v2\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "try:\n",
    "    args = parser.parse_args()\n",
    "except SystemExit:\n",
    "    args = argparse.Namespace(gpu=0)  # Default to 0 if running in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53903e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device('cuda:{}'.format(args.gpu) if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5255805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d18b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LST files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced21d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# from utils.latex import crop_to_formula, renderedLaTeXLabelstr2Formula, display_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3d7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7eef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames_df = pd.read_csv(r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_train.lst\", index_col = 0, header = None, sep = \" \")\n",
    "val_filenames_df = pd.read_csv(r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_validate.lst\", index_col = 0, header = None, sep = \" \")\n",
    "formulas = open(r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_formulas.lst\", encoding = \"ISO-8859-1\", newline=\"\\n\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57b1835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training formulas:  83884\n",
      "Number of validation formulas:  9320\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training formulas: \", len(train_filenames_df))\n",
    "print(\"Number of validation formulas: \", len(val_filenames_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be49f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 998\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(formula) for formula in formulas])\n",
    "print(\"Max length:\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96ccfd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.latex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mskl\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m renderedLaTeXDataset, set_seed\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_tests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_renderedLaTeXDataset\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\LaTeX_OCR-main\\LaTeX_OCR-main\\data\\datasets.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlatex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m crop_to_formula, renderedLaTeXLabelstr2Formula, display_formula\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.latex'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl\n",
    "from data.datasets import renderedLaTeXDataset, set_seed\n",
    "from data.dataset_tests import test_renderedLaTeXDataset\n",
    "    \n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74f2cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 4 # 10 gigs of Vram -> 4, <5 gigs of vram -> 2\n",
    "SHUFFLE_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45f70897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch as t\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)  # If using CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8888bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.datasets import renderedLaTeXDataset, set_seed\n",
    "# from data.dataset_tests import test_renderedLaTeXDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# from utils.latex import crop_to_formula, renderedLaTeXLabelstr2Formula, display_formula\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl\n",
    "import torch as t\n",
    "\n",
    "class renderedLaTeXDataset(Dataset):\n",
    "    def __init__(self, image_folder, lst_file, formulas_file, processor, device, cutoff = None):\n",
    "        self.image_folder = image_folder\n",
    "        self.lst_file = lst_file\n",
    "        self.formulas_file = formulas_file\n",
    "        self.train_filenames_df = pd.read_csv(self.lst_file, sep=\" \", index_col = 0, header = None)\n",
    "        self.formulas = open(self.formulas_file, encoding = \"ISO-8859-1\", newline=\"\\n\").readlines()\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.cutoff = cutoff if cutoff else len(self.train_filenames_df)\n",
    "        if cutoff is not None:\n",
    "            self.train_filenames_df = self.train_filenames_df.iloc[:self.cutoff]\n",
    "            self.formulas = self.formulas[:self.cutoff]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.cutoff\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.train_filenames_df.iloc[idx, 0] + \".png\")\n",
    "        image = Image.open(img_name).convert('RGBA')\n",
    "        image = crop_to_formula(image)\n",
    "        inputs = self.processor(images = image,  padding = \"max_length\", return_tensors=\"pt\").to(self.device)\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].squeeze() # Get rid of batch dimension since the dataloader will batch it for us.\n",
    "\n",
    "        formula_idx = self.train_filenames_df.index[idx]\n",
    "        caption = renderedLaTeXLabelstr2Formula(self.formulas[formula_idx])\n",
    "        caption = self.processor.tokenizer.encode(\n",
    "            caption, return_tensors=\"pt\", padding = \"max_length\", max_length = 512, truncation = True, # Tweak this\n",
    "            ).to(self.device).squeeze()\n",
    "        \n",
    "        return inputs, caption\n",
    "    \n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "    skl.utils.check_random_state(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34241fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit goes to https://www.kaggle.com/code/kalikichandu/preprossing-inkml-to-png-files for original code.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import collections\n",
    "\n",
    "def get_traces_data(inkml_file_abs_path):\n",
    "    \n",
    "\n",
    "    traces_data = []\n",
    "    \n",
    "    tree = ET.parse(inkml_file_abs_path)\n",
    "    root = tree.getroot()\n",
    "    doc_namespace = \"{http://www.w3.org/2003/InkML}\"\n",
    "\n",
    "#   'Stores traces_all with their corresponding id'\n",
    "    traces_all = [{'id': trace_tag.get('id'),\n",
    "    \t\t\t\t\t'coords': [[round(float(axis_coord)) if float(axis_coord).is_integer() else round(float(axis_coord) * 10000) \\\n",
    "    \t\t\t\t\t\t\t\t\tfor axis_coord in coord[1:].split(' ')] if coord.startswith(' ') \\\n",
    "    \t\t\t\t\t\t\t\telse [round(float(axis_coord)) if float(axis_coord).is_integer() else round(float(axis_coord) * 10000) \\\n",
    "    \t\t\t\t\t\t\t\t\tfor axis_coord in coord.split(' ')] \\\n",
    "    \t\t\t\t\t\t\tfor coord in (trace_tag.text).replace('\\n', '').split(',')]} \\\n",
    "    \t\t\t\t\t\t\tfor trace_tag in root.findall(doc_namespace + 'trace')]\n",
    "\n",
    "#   'Sort traces_all list by id to make searching for references faster'\n",
    "    traces_all.sort(key=lambda trace_dict: int(trace_dict['id']))\n",
    "\n",
    "#   'Always 1st traceGroup is a redundant wrapper'\n",
    "    traceGroupWrapper = root.find(doc_namespace + 'traceGroup')\n",
    "\n",
    "    if traceGroupWrapper is not None:\n",
    "        for traceGroup in traceGroupWrapper.findall(doc_namespace + 'traceGroup'):\n",
    "\n",
    "            label = traceGroup.find(doc_namespace + 'annotation').text\n",
    "\n",
    "#    'traces of the current traceGroup'\n",
    "            traces_curr = []\n",
    "            for traceView in traceGroup.findall(doc_namespace + 'traceView'):\n",
    "\n",
    "#     'Id reference to specific trace tag corresponding to currently considered label'\n",
    "                traceDataRef = int(traceView.get('traceDataRef'))\n",
    "\n",
    "#     'Each trace is represented by a list of coordinates to connect'\n",
    "                single_trace = traces_all[traceDataRef]['coords']\n",
    "                traces_curr.append(single_trace)\n",
    "\n",
    "            traces_data.append({'label': label, 'trace_group': traces_curr})\n",
    "\n",
    "    else:\n",
    "#             'Consider Validation data that has no labels'\n",
    "        [traces_data.append({'trace_group': [trace['coords']]}) for trace in traces_all]\n",
    "\n",
    "    return traces_data\n",
    "\n",
    "def get_gt(inkml_file_abs_path):\n",
    "    tree = ET.parse(inkml_file_abs_path)\n",
    "    root = tree.getroot()\n",
    "    doc_namespace = \"{http://www.w3.org/2003/InkML}\"\n",
    "    annotation = root.find(f\".//{doc_namespace}annotation[@type='truth']\")\n",
    "    if annotation is not None:\n",
    "        truth = annotation.text\n",
    "    else: raise Exception(\"No truth annotation found.\")\n",
    "    return truth\n",
    "def inkml2img(input_path, output_path):\n",
    "    traces = get_traces_data(input_path)\n",
    "    if not traces:\n",
    "        print(f\"No traces found for {input_path}.\")\n",
    "        return  # Exit if no traces found\n",
    "\n",
    "    path = os.path.basename(input_path).split('.')[0] + '_'\n",
    "    file_name = 0\n",
    "    plt.axis('off')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.gca().set_xticks([])\n",
    "    plt.gca().set_yticks([])\n",
    "\n",
    "    for elem in traces:\n",
    "        ls = elem['trace_group']\n",
    "        for subls in ls:\n",
    "            data = np.array(subls)\n",
    "            if data.shape[1] > 2:\n",
    "                data = data[:, :2]\n",
    "            x, y = zip(*data)\n",
    "            plt.plot(x, y, linewidth=2, c='black')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_path, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {output_path}: {e}\")\n",
    "        return  # Exit on error\n",
    "\n",
    "    output_file = os.path.join(output_path, f\"{path}{file_name}.png\")\n",
    "    plt.savefig(output_file, bbox_inches='tight', dpi=100)\n",
    "    print(f\"Saved image: {output_file}\")  # Debug info for saved files\n",
    "    plt.gcf().clear()\n",
    "\n",
    "\n",
    "\n",
    "# def inkml2img(input_path, output_path):\n",
    "# #     print(input_path)\n",
    "# #     print(pwd)\n",
    "#     traces = get_traces_data(input_path)\n",
    "# #     print(traces)\n",
    "#     path = input_path.split('/')\n",
    "#     path = path[len(path)-1].split('.')\n",
    "#     path = path[0]+'_'\n",
    "#     file_name = 0\n",
    "#     # Get rid of all matplotlib elements\n",
    "#     plt.axis('off')\n",
    "#     # plt.gca().set_position([0, 0, 1, 1])\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.gca().set_aspect('equal', adjustable='box')\n",
    "#     # plt.gca().set_axis_off()\n",
    "#     plt.gca().set_xticks([])\n",
    "#     plt.gca().set_yticks([])\n",
    "    \n",
    "#     for elem in traces:\n",
    "        \n",
    "# #         print(elem)\n",
    "# #         print('-------------------------')\n",
    "# #         print(elem['label'])\n",
    "#         ls = elem['trace_group']\n",
    "#         output_path = output_path  \n",
    "        \n",
    "#         for subls in ls:\n",
    "# #             print(subls)\n",
    "            \n",
    "#             data = np.array(subls)\n",
    "#             # raise Exception(data)\n",
    "#             if data.shape[1] > 2:\n",
    "#                 data = data[:, :2]\n",
    "#             x,y=zip(*data)\n",
    "#             plt.plot(x,y,linewidth=2,c='black')\n",
    "#     try:\n",
    "#         os.mkdir(output_path)\n",
    "#     except OSError:\n",
    "# #             print (\"Folder %s Already Exists\" % ind_output_path)\n",
    "# #             print(OSError.strerror)\n",
    "#         pass\n",
    "#     else:\n",
    "# #             print (\"Successfully created the directory %s \" % ind_output_path)\n",
    "#         pass\n",
    "# #         print(ind_output_path+'/'+path+str(file_name)+'.png')\n",
    "    input_path_safe = input_path.replace('/', '_') + '_'\n",
    "    if(os.path.isfile(output_path+'/'+input_path_safe+str(file_name)+'.png')):\n",
    "        # print('1111')\n",
    "        file_name += 1\n",
    "        plt.savefig(output_path+'/'+input_path_safe+str(file_name)+'.png', bbox_inches='tight', dpi=100)\n",
    "    else:\n",
    "        plt.savefig(output_path+'/'+input_path_safe+str(file_name)+'.png', bbox_inches='tight', dpi=100)\n",
    "    plt.gcf().clear()\n",
    "\n",
    "def ink2img_folder(input_paths, output_path):\n",
    "    labels = collections.defaultdict(list)\n",
    "    for input_path in input_paths:\n",
    "        input_path_safe = input_path.replace('/', '_') + '_'\n",
    "        files = os.listdir(input_path)\n",
    "        # ignore all files that don't have the .inkML extension\n",
    "        files = [file for file in files if file.endswith('.inkml')]\n",
    "        for file in tqdm(files):\n",
    "        #     print(file)\n",
    "            if output_path[-1] != \"/\": output_path += \"/\"\n",
    "            inkML_path = os.path.join(input_path, file)\n",
    "            try: \n",
    "                labels[\"label\"].append(get_gt(inkML_path))\n",
    "                labels[\"name\"].append((input_path_safe+file).replace('.','_').replace('_inkml', '.inkml')+'_0.png')\n",
    "                inkml2img(inkML_path, output_path)\n",
    "            except: print(\"Error with file: \" + str(file) + \" in folder: \" + str(input_path) + \". Don't worry, this is expected (though there should only be max 2 or 3!).\")\n",
    "    pd.DataFrame(labels).to_csv(output_path + \"labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49a598a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import re\n",
    "import numpy as np\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "def crop_to_formula(image, padding = 30):\n",
    "    # Image: 4 channel image with alpha.\n",
    "    # Convert black pixels to white pixels.\n",
    "    data = np.array(image)\n",
    "    red, green, blue, alpha = data.T\n",
    "    black_areas = (red < 10) & (blue < 10) & (green < 10)\n",
    "    # Convert alpha to white.\n",
    "    data[..., -1] = 255\n",
    "    # Crop a box around the area that contains black pixels.\n",
    "    coords = np.argwhere(black_areas)\n",
    "    x0, y0 = coords.min(axis=0)\n",
    "    x1, y1 = coords.max(axis=0) + 1\n",
    "    # Add padding.\n",
    "    x0 = max(0, x0 - padding)\n",
    "    y0 = max(0, y0 - padding)\n",
    "    x1 = min(image.width, x1 + padding)\n",
    "    y1 = min(image.height, y1 + padding)\n",
    "    image = Image.fromarray(data[y0:y1, x0:x1])\n",
    "    return image.convert('RGB')\n",
    "\n",
    "def renderedLaTeXLabelstr2Formula(label: str):\n",
    "    # We're matching \\\\label{...whatever} and removing it\n",
    "    label = re.sub(r\"\\\\label\\{[^\\}]*\\}\", \"\", label)\n",
    "    # We match \\, and remove it.\n",
    "    label = re.sub(r\"\\\\,\", \"\", label)\n",
    "    return label\n",
    "\n",
    "def display_formula(latex: str):\n",
    "    # Remove \\mbox{...} - not supported by the inline MathJax renderer\n",
    "    parsed_latex = re.sub(r\"\\\\mbox\\{[^\\}]*\\}\", \"\", latex)\n",
    "    display(Math(parsed_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ca9e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "def test_renderedLaTeXDataset(dataset, processor):\n",
    "    \n",
    "    iter_ = iter(dataset)\n",
    "    inputs, captions = next(iter_)\n",
    "    inputs_2, captions_2 = next(iter_)\n",
    "    assert ''.join(processor.batch_decode(captions, skip_special_tokens=True)) != ''.join(processor.batch_decode(captions_2, skip_special_tokens=True)), \"Passed dataset yields repeat captions.\"\n",
    "\n",
    "    print(\"renderedLaTeXDataset tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b0fcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prana\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renderedLaTeXDataset tests passed.\n",
      "renderedLaTeXDataset tests passed.\n",
      "Number of training samples: 83884\n",
      "Number of validation samples: 9320\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "optimizer = t.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.v2.RandomAffine(degrees = 5,\n",
    "                               scale = (0.7, 1.1),\n",
    "                               shear = 30),\n",
    "    transforms.v2.ColorJitter(brightness = 0.2,\n",
    "                              contrast = 0.2,\n",
    "                              saturation = 0.2,\n",
    "                              hue = 0.1)\n",
    "])\n",
    "\n",
    "train_ds = renderedLaTeXDataset(image_folder = r\"C:\\Users\\prana\\Downloads\\56198\\formula_images\\formula_images\", \n",
    "                                lst_file = r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_train.lst\", \n",
    "                                formulas_file = r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_formulas.lst\", \n",
    "\n",
    "                                device = device,\n",
    "                                processor = processor,\n",
    "                            )\n",
    "val_ds = renderedLaTeXDataset(image_folder = r\"C:\\Users\\prana\\Downloads\\56198\\formula_images\\formula_images\",\n",
    "                                lst_file = r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_validate.lst\",\n",
    "                                formulas_file = r\"C:\\Users\\prana\\Downloads\\56198\\im2latex_formulas.lst\",\n",
    "                                device = device,\n",
    "                                processor = processor)\n",
    "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = SHUFFLE_DATASET, num_workers = 0)\n",
    "val_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = False, num_workers = 0)\n",
    "\n",
    "test_renderedLaTeXDataset(train_ds, processor = processor)\n",
    "test_renderedLaTeXDataset(val_ds, processor = processor)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_ds))\n",
    "print(\"Number of validation samples:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d1d0253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "118ef664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|                                                                             | 0/20971 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.40 GiB is allocated by PyTorch, and 237.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m t\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mfloat16, enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(pixel_values \u001b[38;5;241m=\u001b[39m pixel_values,\n\u001b[0;32m     12\u001b[0m                     labels \u001b[38;5;241m=\u001b[39m captions)\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     14\u001b[0m     history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:615\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m    611\u001b[0m         labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m    616\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    617\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m    618\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    619\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    620\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m    621\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    622\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    623\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    624\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    625\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_decoder,\n\u001b[0;32m    627\u001b[0m )\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\models\\trocr\\modeling_trocr.py:912\u001b[0m, in \u001b[0;36mTrOCRForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    909\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m    913\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    914\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    915\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    916\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    917\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    918\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[0;32m    919\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    920\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    921\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    922\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    923\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    924\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    925\u001b[0m )\n\u001b[0;32m    927\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projection(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    929\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\models\\trocr\\modeling_trocr.py:672\u001b[0m, in \u001b[0;36mTrOCRDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    659\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    660\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    661\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m         use_cache,\n\u001b[0;32m    670\u001b[0m     )\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 672\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    673\u001b[0m         hidden_states,\n\u001b[0;32m    674\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    675\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    676\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    677\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    678\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    679\u001b[0m             cross_attn_head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m cross_attn_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    680\u001b[0m         ),\n\u001b[0;32m    681\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    682\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\models\\trocr\\modeling_trocr.py:379\u001b[0m, in \u001b[0;36mTrOCRDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    378\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_attn(\n\u001b[0;32m    380\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    381\u001b[0m     key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    382\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    383\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[0;32m    384\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mcross_attn_past_key_value,\n\u001b[0;32m    385\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    386\u001b[0m )\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\Lib\\site-packages\\transformers\\models\\trocr\\modeling_trocr.py:231\u001b[0m, in \u001b[0;36mTrOCRAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    228\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mproj_shape)\n\u001b[0;32m    230\u001b[0m src_len \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.40 GiB is allocated by PyTorch, and 237.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "history = []; val_history = []; val_timesteps = []\n",
    "ema_loss = None; ema_alpha = 0.95\n",
    "scaler = t.cuda.amp.GradScaler(enabled = True)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    with tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\") as pbar:\n",
    "        for batch, captions in pbar:\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with t.autocast(device_type = \"cuda\", dtype = t.float16, enabled = True):\n",
    "                outputs = model(pixel_values = pixel_values,\n",
    "                                labels = captions)\n",
    "                loss = outputs.loss\n",
    "                history.append(loss.item())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if ema_loss is None: ema_loss = loss.item()\n",
    "            else: ema_loss = ema_loss * ema_alpha + loss.item() * (1 - ema_alpha)\n",
    "            pbar.set_postfix(loss=ema_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        val_losses = []\n",
    "        for batch, captions in tqdm(val_dl):\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "            outputs = model(pixel_values = pixel_values,\n",
    "                            labels = captions)\n",
    "            val_losses.append(outputs.loss.item())\n",
    "        print(f\"Validation loss: {np.mean(val_losses)}\")\n",
    "        val_history.append(np.mean(val_losses))\n",
    "        val_timesteps.append(len(history) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b98b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(r\"C:/Users/prana/Downloads/Models/trocr-large-rendered-im2latex\")\n",
    "processor.save_pretrained(r\"C:/Users/prana/Downloads/Models/trocr-large-rendered-im2latex\")\n",
    "t.save(history, r\"C:/Users/prana/Downloads/Models/trocr-large-rendered-im2latex/history.pt\")\n",
    "t.save(val_history, r\"C:/Users/prana/Downloads/Models/trocr-large-rendered-im2latex/val_history.pt\")\n",
    "t.save(val_timesteps, r\"C:/Users/prana/Downloads/Models/val_timesteps.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
